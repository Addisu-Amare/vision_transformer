{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCq6aF0rVLtb"
      },
      "source": [
        "# Seminar 5 & 6\n",
        "\n",
        "by Hekmat Taherinejad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyyOEwvgGk_U"
      },
      "source": [
        "# Recurrent neural networks (RNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKctr4a1sLP-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gq_BGEctsYuE"
      },
      "outputs": [],
      "source": [
        "# Hyper-parameters\n",
        "\n",
        "num_classes = 10\n",
        "n_iters = 3000\n",
        "batch_size = 100\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCAKjeHmO1SP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebaba0b9-ef54-4418-eeba-f27de4f47c3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 74168834.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 56658416.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 27897120.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4613835.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data',\n",
        "                                           train=True,\n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data',\n",
        "                                          train=False,\n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader: These loaders handle the shuffling and batching of the dataset during training and testing, respectively.\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e2Y7Emusc-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e658815-3ffc-49d5-c8be-a745695ad0bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([60000, 28, 28])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset.train_data.size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WANwnjnOsgk8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26a9a8d4-f920-49ff-92f5-a99e87b9db2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([60000])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset.train_labels.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvTAmNjusvrv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b672913e-13e0-41bb-b00d-c9c0fd766d65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 28, 28])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:80: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n"
          ]
        }
      ],
      "source": [
        "print(test_dataset.test_data.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dr1bnT7IszH9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adf11dd1-b25b-4664-915d-92c8d3629795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:70: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ]
        }
      ],
      "source": [
        "print(test_dataset.test_labels.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AT-UP4hxeIv"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/ritchieng/deep-learning-wizard/dc6fb5ccfaf6ca4760f673c2384330d5b2069bf2/docs/deep_learning/practical_pytorch/images/rnn4n.png\" alt=\"Deep Recurrent Neural Networks\">"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = n_iters / (len(train_dataset) / batch_size) # This calculation ensures that the model goes through the entire dataset\n",
        "num_epochs = int(num_epochs)\n",
        "learning_rate = 0.001\n",
        "\n",
        "input_size = 28\n",
        "sequence_length = 28\n",
        "hidden_size = 128\n",
        "num_layers = 1"
      ],
      "metadata": {
        "id": "Ec4PMG0uUI9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXEL77iI5ZQz"
      },
      "outputs": [],
      "source": [
        "# Fully connected neural network with one hidden layer\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        # Hidden dimensions\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "\n",
        "        # Number of hidden layers\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # TODO\n",
        "        # Building your RNN\n",
        "        # batch_first=True causes input/output tensors to be of shape:\n",
        "        # (batch_dim, seq_dim, input_dim) -> x needs to be: (batch_size, seq, input_size)\n",
        "        self.rnn =nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # TODO\n",
        "        # Initialize hidden state with zeros\n",
        "        # (layer_dim, batch_size, hidden_dim)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # x: (n, 28, 28), h0: (2, n, 128)\n",
        "\n",
        "        # Forward propagate RNN\n",
        "        out, _ = self.rnn(x, h0)\n",
        "\n",
        "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "        # out: (n, 28, 128)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = out[:, -1, :]\n",
        "        # out: (n, 128)\n",
        "\n",
        "        out = self.fc(out)\n",
        "        # out: (n, 10)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhKXdRM0PCu4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00bde25a-c5f6-4b12-98b6-79a28b8ccf00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 1.2213\n",
            "Epoch [1/5], Step [200/600], Loss: 1.2950\n",
            "Epoch [1/5], Step [300/600], Loss: 0.9491\n",
            "Epoch [1/5], Step [400/600], Loss: 0.8533\n",
            "Epoch [1/5], Step [500/600], Loss: 1.1332\n",
            "Epoch [1/5], Step [600/600], Loss: 0.5480\n",
            "Epoch [2/5], Step [100/600], Loss: 0.5500\n",
            "Epoch [2/5], Step [200/600], Loss: 0.4251\n",
            "Epoch [2/5], Step [300/600], Loss: 0.4668\n",
            "Epoch [2/5], Step [400/600], Loss: 0.2962\n",
            "Epoch [2/5], Step [500/600], Loss: 0.3938\n",
            "Epoch [2/5], Step [600/600], Loss: 0.3181\n",
            "Epoch [3/5], Step [100/600], Loss: 0.2425\n",
            "Epoch [3/5], Step [200/600], Loss: 0.2255\n",
            "Epoch [3/5], Step [300/600], Loss: 0.3977\n",
            "Epoch [3/5], Step [400/600], Loss: 0.3861\n",
            "Epoch [3/5], Step [500/600], Loss: 0.2767\n",
            "Epoch [3/5], Step [600/600], Loss: 0.2349\n",
            "Epoch [4/5], Step [100/600], Loss: 0.2043\n",
            "Epoch [4/5], Step [200/600], Loss: 0.4115\n",
            "Epoch [4/5], Step [300/600], Loss: 0.2341\n",
            "Epoch [4/5], Step [400/600], Loss: 0.1383\n",
            "Epoch [4/5], Step [500/600], Loss: 0.2944\n",
            "Epoch [4/5], Step [600/600], Loss: 0.2109\n",
            "Epoch [5/5], Step [100/600], Loss: 0.1872\n",
            "Epoch [5/5], Step [200/600], Loss: 0.2258\n",
            "Epoch [5/5], Step [300/600], Loss: 0.1541\n",
            "Epoch [5/5], Step [400/600], Loss: 0.1713\n",
            "Epoch [5/5], Step [500/600], Loss: 0.2037\n",
            "Epoch [5/5], Step [600/600], Loss: 0.1282\n",
            "Accuracy of the network on the 10000 test images: 95.67 %\n"
          ]
        }
      ],
      "source": [
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # origin shape: [N, 1, 28, 28]\n",
        "        # resized: [N, 28, 28]\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG_ZieWB8nwk"
      },
      "source": [
        "### Deep RNN\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/ritchieng/deep-learning-wizard/dc6fb5ccfaf6ca4760f673c2384330d5b2069bf2/docs/deep_learning/practical_pytorch/images/rnn6.png\" alt=\"Deep Recurrent Neural Networks\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YuNL0F9-Y1RP"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "# Increase number of Layers\n",
        "num_layers = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gYLbLmi4wDL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4bdb20a-ba08-4252-c5a1-20aa29a992ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 1.1134\n",
            "Epoch [1/5], Step [200/600], Loss: 0.9162\n",
            "Epoch [1/5], Step [300/600], Loss: 0.5617\n",
            "Epoch [1/5], Step [400/600], Loss: 0.3885\n",
            "Epoch [1/5], Step [500/600], Loss: 0.4364\n",
            "Epoch [1/5], Step [600/600], Loss: 0.3897\n",
            "Epoch [2/5], Step [100/600], Loss: 0.3577\n",
            "Epoch [2/5], Step [200/600], Loss: 0.1870\n",
            "Epoch [2/5], Step [300/600], Loss: 0.1890\n",
            "Epoch [2/5], Step [400/600], Loss: 0.1543\n",
            "Epoch [2/5], Step [500/600], Loss: 0.1740\n",
            "Epoch [2/5], Step [600/600], Loss: 0.2211\n",
            "Epoch [3/5], Step [100/600], Loss: 0.2751\n",
            "Epoch [3/5], Step [200/600], Loss: 0.2018\n",
            "Epoch [3/5], Step [300/600], Loss: 0.2035\n",
            "Epoch [3/5], Step [400/600], Loss: 0.1073\n",
            "Epoch [3/5], Step [500/600], Loss: 0.0875\n",
            "Epoch [3/5], Step [600/600], Loss: 0.2582\n",
            "Epoch [4/5], Step [100/600], Loss: 0.2509\n",
            "Epoch [4/5], Step [200/600], Loss: 0.1978\n",
            "Epoch [4/5], Step [300/600], Loss: 0.0949\n",
            "Epoch [4/5], Step [400/600], Loss: 0.2005\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0550\n",
            "Epoch [4/5], Step [600/600], Loss: 0.1466\n",
            "Epoch [5/5], Step [100/600], Loss: 0.1819\n",
            "Epoch [5/5], Step [200/600], Loss: 0.1955\n",
            "Epoch [5/5], Step [300/600], Loss: 0.1401\n",
            "Epoch [5/5], Step [400/600], Loss: 0.1015\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0969\n",
            "Epoch [5/5], Step [600/600], Loss: 0.1201\n",
            "Accuracy of the network on the 10000 test images: 96.18 %\n"
          ]
        }
      ],
      "source": [
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # origin shape: [N, 1, 28, 28]\n",
        "        # resized: [N, 28, 28]\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtxLH0Mb9YRV"
      },
      "source": [
        "### Bidirectional RNN\n",
        "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20230302163012/Bidirectional-Recurrent-Neural-Network-2.png\" alt=\"Bidirectional Recurrent Neural Networks\" width=\"600\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gcbmsAz0UXli"
      },
      "outputs": [],
      "source": [
        "num_layers = 1\n",
        "class BiRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(BiRNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # TODO: Add \"bidirectional=True\" argument to the RNN model\n",
        "        self.rnn =nn.RNN(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # Multiply by 2 for bidirectional\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # TODO\n",
        "        # Initialize hidden state with zeros\n",
        "        # num_layers * 2\n",
        "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "t3RGNbf-zVPq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8350d8ea-dd76-475f-fadd-bbe0d57ca463"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 1.2801\n",
            "Epoch [1/5], Step [200/600], Loss: 1.0338\n",
            "Epoch [1/5], Step [300/600], Loss: 0.8040\n",
            "Epoch [1/5], Step [400/600], Loss: 0.8008\n",
            "Epoch [1/5], Step [500/600], Loss: 0.7756\n",
            "Epoch [1/5], Step [600/600], Loss: 0.6269\n",
            "Epoch [2/5], Step [100/600], Loss: 0.5818\n",
            "Epoch [2/5], Step [200/600], Loss: 0.3815\n",
            "Epoch [2/5], Step [300/600], Loss: 0.3869\n",
            "Epoch [2/5], Step [400/600], Loss: 0.4511\n",
            "Epoch [2/5], Step [500/600], Loss: 0.3980\n",
            "Epoch [2/5], Step [600/600], Loss: 0.3471\n",
            "Epoch [3/5], Step [100/600], Loss: 0.4219\n",
            "Epoch [3/5], Step [200/600], Loss: 0.3822\n",
            "Epoch [3/5], Step [300/600], Loss: 0.2486\n",
            "Epoch [3/5], Step [400/600], Loss: 0.3476\n",
            "Epoch [3/5], Step [500/600], Loss: 0.3780\n",
            "Epoch [3/5], Step [600/600], Loss: 0.2647\n",
            "Epoch [4/5], Step [100/600], Loss: 0.2536\n",
            "Epoch [4/5], Step [200/600], Loss: 0.2840\n",
            "Epoch [4/5], Step [300/600], Loss: 0.4722\n",
            "Epoch [4/5], Step [400/600], Loss: 0.2085\n",
            "Epoch [4/5], Step [500/600], Loss: 0.1898\n",
            "Epoch [4/5], Step [600/600], Loss: 0.3335\n",
            "Epoch [5/5], Step [100/600], Loss: 0.1680\n",
            "Epoch [5/5], Step [200/600], Loss: 0.2775\n",
            "Epoch [5/5], Step [300/600], Loss: 0.3135\n",
            "Epoch [5/5], Step [400/600], Loss: 0.0990\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0943\n",
            "Epoch [5/5], Step [600/600], Loss: 0.1617\n",
            "Accuracy of the network on the 10000 test images: 95.04 %\n"
          ]
        }
      ],
      "source": [
        "model = BiRNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # origin shape: [N, 1, 28, 28]\n",
        "        # resized: [N, 28, 28]\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwT9r8kI2aIH"
      },
      "source": [
        "For more information:\n",
        "[deeplearningwizard](https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipnCuess_nkN"
      },
      "source": [
        "# LSTM\n",
        "\n",
        "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\" alt=\"LSTM\" width=\"600\">\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://www.deeplearningwizard.com/deep_learning/practical_pytorch/images/lstm2.png\" alt=\"LSTM\" width=\"800\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3TgS3Ct85ZQ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be5c0088-1b4f-4fc9-fcf6-ea735852ce10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 0.7685\n",
            "Epoch [1/5], Step [200/600], Loss: 0.5941\n",
            "Epoch [1/5], Step [300/600], Loss: 0.2579\n",
            "Epoch [1/5], Step [400/600], Loss: 0.4308\n",
            "Epoch [1/5], Step [500/600], Loss: 0.3272\n",
            "Epoch [1/5], Step [600/600], Loss: 0.1775\n",
            "Epoch [2/5], Step [100/600], Loss: 0.1761\n",
            "Epoch [2/5], Step [200/600], Loss: 0.1087\n",
            "Epoch [2/5], Step [300/600], Loss: 0.1787\n",
            "Epoch [2/5], Step [400/600], Loss: 0.2940\n",
            "Epoch [2/5], Step [500/600], Loss: 0.1122\n",
            "Epoch [2/5], Step [600/600], Loss: 0.1067\n",
            "Epoch [3/5], Step [100/600], Loss: 0.0744\n",
            "Epoch [3/5], Step [200/600], Loss: 0.0936\n",
            "Epoch [3/5], Step [300/600], Loss: 0.0784\n",
            "Epoch [3/5], Step [400/600], Loss: 0.1471\n",
            "Epoch [3/5], Step [500/600], Loss: 0.0537\n",
            "Epoch [3/5], Step [600/600], Loss: 0.0688\n",
            "Epoch [4/5], Step [100/600], Loss: 0.0291\n",
            "Epoch [4/5], Step [200/600], Loss: 0.0859\n",
            "Epoch [4/5], Step [300/600], Loss: 0.0745\n",
            "Epoch [4/5], Step [400/600], Loss: 0.0880\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0455\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0654\n",
            "Epoch [5/5], Step [100/600], Loss: 0.0188\n",
            "Epoch [5/5], Step [200/600], Loss: 0.1060\n",
            "Epoch [5/5], Step [300/600], Loss: 0.1081\n",
            "Epoch [5/5], Step [400/600], Loss: 0.1121\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0462\n",
            "Epoch [5/5], Step [600/600], Loss: 0.0758\n",
            "Accuracy of the network on the 10000 test images: 97.66 %\n"
          ]
        }
      ],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        # Hidden dimensions\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Number of hidden layers\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Building your LSTM\n",
        "        # batch_first=True causes input/output tensors to be of shape\n",
        "        # (batch_dim, seq_dim, feature_dim)\n",
        "        # -> x needs to be: (batch_size, seq, input_size)\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Readout layer\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state with zeros\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)  # Shape: (num_layers, batch_size, hidden_size)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)  # Shape: (num_layers, batch_size, hidden_size)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "        # out: (n, 28, 128)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = out[:, -1, :]  # Extract the output of the last time step\n",
        "        # out: (n, 128)\n",
        "\n",
        "        out = self.fc(out)\n",
        "        # out: (n, 10)\n",
        "        return out\n",
        "\n",
        "\n",
        "model = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # origin shape: [N, 1, 28, 28]\n",
        "        # resized: [N, 28, 28]\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noTZk1zF_8Zg"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "<img src=\"https://www.baeldung.com/wp-content/uploads/sites/4/2022/01/bilstm-1-1024x384.png\" alt=\"BiLSTM\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Initialize bidirectional LSTM\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # Multiply by 2 for bidirectional\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Set initial hidden states and cell states\n",
        "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)  # Shape: (num_layers * num_directions, batch_size, hidden_size)\n",
        "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)  # Shape: (num_layers * num_directions, batch_size, hidden_size)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # out: tensor of shape (batch_size, seq_length, hidden_size*num_directions)\n",
        "        # out: (n, seq, hidden_size*2)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = out[:, -1, :]  # Extract the output of the last time step\n",
        "        # out: (n, hidden_size*2)\n",
        "\n",
        "        out = self.fc(out)\n",
        "        # out: (n, num_classes)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "-zcfSB3GpNRs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Y8bakRZ_WhVC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fb98f17-3b6f-42ee-e76b-9dbd5e8cde8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 0.7727\n",
            "Epoch [1/5], Step [200/600], Loss: 0.3035\n",
            "Epoch [1/5], Step [300/600], Loss: 0.3172\n",
            "Epoch [1/5], Step [400/600], Loss: 0.3497\n",
            "Epoch [1/5], Step [500/600], Loss: 0.1562\n",
            "Epoch [1/5], Step [600/600], Loss: 0.2453\n",
            "Epoch [2/5], Step [100/600], Loss: 0.1293\n",
            "Epoch [2/5], Step [200/600], Loss: 0.3081\n",
            "Epoch [2/5], Step [300/600], Loss: 0.0880\n",
            "Epoch [2/5], Step [400/600], Loss: 0.1361\n",
            "Epoch [2/5], Step [500/600], Loss: 0.0611\n",
            "Epoch [2/5], Step [600/600], Loss: 0.1257\n",
            "Epoch [3/5], Step [100/600], Loss: 0.0849\n",
            "Epoch [3/5], Step [200/600], Loss: 0.1246\n",
            "Epoch [3/5], Step [300/600], Loss: 0.1182\n",
            "Epoch [3/5], Step [400/600], Loss: 0.0985\n",
            "Epoch [3/5], Step [500/600], Loss: 0.1464\n",
            "Epoch [3/5], Step [600/600], Loss: 0.0697\n",
            "Epoch [4/5], Step [100/600], Loss: 0.1079\n",
            "Epoch [4/5], Step [200/600], Loss: 0.0199\n",
            "Epoch [4/5], Step [300/600], Loss: 0.1342\n",
            "Epoch [4/5], Step [400/600], Loss: 0.1825\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0611\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0352\n",
            "Epoch [5/5], Step [100/600], Loss: 0.1183\n",
            "Epoch [5/5], Step [200/600], Loss: 0.0650\n",
            "Epoch [5/5], Step [300/600], Loss: 0.0463\n",
            "Epoch [5/5], Step [400/600], Loss: 0.0650\n",
            "Epoch [5/5], Step [500/600], Loss: 0.1089\n",
            "Epoch [5/5], Step [600/600], Loss: 0.1418\n",
            "Accuracy of the network on the 10000 test images: 97.91 %\n"
          ]
        }
      ],
      "source": [
        "model = BiLSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # origin shape: [N, 1, 28, 28]\n",
        "        # resized: [N, 28, 28]\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owezC0mp3KO-"
      },
      "source": [
        "For more information:\n",
        "[deeplearningwizard](https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8__oGYD5ZRB"
      },
      "source": [
        "# Building a GPT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgO7yYzc5ZRB"
      },
      "outputs": [],
      "source": [
        "# Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVdoGoLr5ZRB"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln5Pgwk15ZRB"
      },
      "outputs": [],
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i99ib5qk5ZRC"
      },
      "outputs": [],
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3znzMcq5ZRC"
      },
      "outputs": [],
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LKogC1f5ZRC"
      },
      "outputs": [],
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJpZrE6s5ZRC"
      },
      "outputs": [],
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_cRXl4X5ZRC"
      },
      "outputs": [],
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roiSea685ZRD"
      },
      "outputs": [],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpGuXDPh5ZRD"
      },
      "outputs": [],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRxeSUoP5ZRD"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOaDdt4q5ZRD"
      },
      "outputs": [],
      "source": [
        "print(xb) # our input to the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdl5COb0H_1f"
      },
      "source": [
        "### N-gram Language Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XS-I6CA55ZRD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YJq2d6H5ZRD"
      },
      "outputs": [],
      "source": [
        " # create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff32VlD45ZRD"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2A94VMy5ZRD"
      },
      "outputs": [],
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwPjstbc5ZRD"
      },
      "source": [
        "\n",
        "\n",
        "### The mathematical trick in self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWIxuurR5ZRD"
      },
      "outputs": [],
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-fiIAHS5ZRD"
      },
      "outputs": [],
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNTJbDei5ZRE"
      },
      "outputs": [],
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NG1e_Z715ZRE"
      },
      "outputs": [],
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf')) # future can't communicate with past\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow2, xbow3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgz7L2Vq5ZRE"
      },
      "outputs": [],
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yviglybc5ZRH"
      },
      "outputs": [],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaoRRBhuLfCl"
      },
      "outputs": [],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-MqvzltLg9w"
      },
      "outputs": [],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Sz804FrL_hU"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0Nya1MVO-cy"
      },
      "outputs": [],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdu8fUJ75ZRI"
      },
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "\n",
        "* sentence 1: The bank1 of the river.\n",
        "* sentence 2: Money in the bank2.\n",
        "\n",
        "![alt text](https://files.readme.io/298afce-image.png)\n",
        "\n",
        "![alt text](https://files.readme.io/5f8c5fb-image.png)\n",
        "\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_Fa49kU5ZRI"
      },
      "source": [
        "## Attention\n",
        "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.  The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
        "\n",
        "We call our particular attention \"Scaled Dot-Product Attention\".   The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$.  We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQbn0XxJ5ZRI"
      },
      "source": [
        "![alt text](https://raw.githubusercontent.com/harvardnlp/annotated-transformer/master/images/ModalNet-19.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAb2ombp5ZRI"
      },
      "source": [
        "\n",
        "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$.   The keys and values are also packed together into matrices $K$ and $V$.  We compute the matrix of outputs as:\n",
        "\n",
        "$$\n",
        "   \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ML88645ZRI"
      },
      "source": [
        "![alt text](https://lena-voita.github.io/resources/lectures/seq2seq/transformer/qkv_explained-min.png)\n",
        "Source: [Lena Voita's Lecture about Seq2Seq](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "mxc2r7HX_QbS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.scale = self.dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Args:\n",
        "            x: Tensor of shape (batch_size, seq_len, input_dim)\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape (batch_size, seq_len, input_dim)\n",
        "        '''\n",
        "        Batch, length_seq,input_dim = x.shape# Extract batch size, sequence length, and input dimension.\n",
        "        qkv = self.qkv(x).reshape(Batch, length_seq, 3, self.dim)# Perform linear transformation and reshape for queries, keys, and values.\n",
        "        q, k, v = qkv.unbind(2)# Unbind into separate queries, keys, and values.\n",
        "        q = q * self.scale# Scale the queries.\n",
        "        attn = q @ k.transpose(-2, -1)  # Compute attention scores.\n",
        "        attn = attn.softmax(dim=-1)    # Apply softmax to obtain attention weights\n",
        "        x = attn @ v# Compute weighted sum of values using attention weights.\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "M2ybwNCU_w2q"
      },
      "outputs": [],
      "source": [
        "x = torch.ones(11, 12, 8)\n",
        "assert Attention(8)(x).shape == x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI3w-ThWBTSl"
      },
      "source": [
        "# Multi-head attention\n",
        "\n",
        "\n",
        "![](Attention.png)\n",
        "\n",
        "- Divide each vector in a sequence into `num_heads` vectors ($d$ mod `num_heads` = 0)\n",
        "- Apply attention layers independently, concatenate the result\n",
        "$$\\text{head}_i = \\text{Attention}(Q_i, K_i, V_i)$$\n",
        "$$ \\textrm{concat} \\left( \\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h \\right) $$\n",
        "- Apply an extra linear layer to mix independent attention branches\n",
        "- **How to implement without loops?**\n",
        "\n",
        "![alt text](https://uvadlc-notebooks.readthedocs.io/en/latest/_images/multihead_attention.svg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "mQzpOiXBBw0j"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8):\n",
        "        super().__init__()\n",
        "        if dim % num_heads:\n",
        "            raise ValueError('dim % num_heads != 0')\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Args:\n",
        "            x: Tensor of shape (batch_size, seq_len, input_dim)\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape (batch_size, seq_len, input_dim)\n",
        "        '''\n",
        "        # Hint: you might want to use torch.permute function\n",
        "\n",
        "        # Extract batch size, sequence length, and input dimension.\n",
        "\n",
        "\n",
        "        # qkv: 3 × B × num_heads × N × head_dim\n",
        "\n",
        "        # Unbind into separate queries, keys, and values.\n",
        "\n",
        "        # Scale the queries.\n",
        "\n",
        "        # Compute attention scores.\n",
        "\n",
        "        # Apply softmax to obtain attention weights.\n",
        "\n",
        "        # Compute weighted sum of values using attention weights.\n",
        "\n",
        "        # x: B × num_heads × N × head_dim\n",
        "\n",
        "        # Reshape and transpose back to original shape.\n",
        "\n",
        "        # x: B × N × (num_heads × head_dim)\n",
        "\n",
        "\n",
        "        Batch_size, length_seq, input_dim = x.shape# Extract batch size, sequence length, and input dimension.\n",
        "         # Perform linear transformation and reshape for queries, keys, and values.\n",
        "        qkv = self.qkv(x).reshape(Batch_size, length_seq, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        # qkv: 3 × B × num_heads × N × head_dim\n",
        "        q, k, v = qkv.unbind(0) # Unbind into separate queries, keys, and values.\n",
        "        q = q * self.scale# Scale the queries.\n",
        "        attn = q @ k.transpose(-2, -1) # Compute attention scores.\n",
        "        attn = attn.softmax(dim=-1) # Apply softmax to obtain attention weights.\n",
        "        x = attn @ v  # attn: B × num_heads × N × N    v: B × num_heads × N × head_dim,  # Compute weighted sum of values using attention weights.\n",
        "        # B × num_heads × N × head_dim\n",
        "        x = x.transpose(1, 2).reshape(Batch_size, length_seq, input_dim) # Reshape and transpose back to original shape.\n",
        "        # B × N × (num_heads × head_dim)\n",
        "        x = self.proj(x)  # Project the output.\n",
        "\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "OHf9UkyfBx_1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bce5cba5-4785-4cd7-d2ab-5053c68a56f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([11, 12, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "MultiHeadAttention(128, 8)(torch.ones(11, 12, 128)).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5z3D0-2S817"
      },
      "source": [
        "# ViT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "madeIO5PEkvJ"
      },
      "source": [
        "### Einops.rearrange\n",
        "\n",
        "https://github.com/arogozhnikov/einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "dCpqdrLHDc-y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53b9c68b-78c3-4a43-f3bc-86b0f58dd4dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install einops -q\n",
        "from einops import rearrange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "n06zLD4uEq62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "924f7ad6-7dca-48d1-c581-feef84a3bdda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 8, 4, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Transposition:\n",
        "rearrange(torch.arange(1024).reshape(2, 4, 8, 16), 'aa b c d -> d c b aa').shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ikd0GDn-EsBz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eed77709-adde-4fd7-eeec-d98e6c3f5f64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "res = rearrange(torch.arange(30).reshape(5, 6), 'a (b c) -> a b c', b=2, c=3)\n",
        "res.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44-jNoTWE7Ij"
      },
      "source": [
        "## Patches crafting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "xT7qgkP0E67w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2870c5ca-8ffe-4edd-ae40-32c597709aa9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 1089, 192])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "! python3 -m pip install einops -q\n",
        "from einops import rearrange\n",
        "\n",
        "def img2patches(img, patch_size=8):\n",
        "    '''\n",
        "    Args:\n",
        "        img: (batch_size, c, h, w) Tensor\n",
        "\n",
        "    Returns:\n",
        "        (batch_size, num_patches, vectorized_patch) Tensor\n",
        "    '''\n",
        "    # Your code is here\n",
        "    # Rearrange the image tensor to extract patches\n",
        "    batch_size, channels, height, width = img.shape\n",
        "    num_patches = (height // patch_size) * (width // patch_size)\n",
        "    patches = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size)\n",
        "    return patches\n",
        "img2patches(torch.ones(2,3,264,264)).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YswGSOazAorx"
      },
      "source": [
        "##  Build ViT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqEx_2kVAO4I"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/oseledets/dl2023/b5018a354b1a10e7f498d3a8649f604f4d63d920/seminars/seminar-9/vit.webp\" alt=\"ViT\" width=\"600\">\n",
        "\n",
        "\n",
        "* Split an image into patches\n",
        "\n",
        "* Flatten the patches\n",
        "\n",
        "* Produce lower-dimensional linear embeddings from the flattened patches\n",
        "\n",
        "* Add positional embeddings\n",
        "\n",
        "* Feed the sequence as an input to a standard transformer encoder\n",
        "\n",
        "* Pretrain the model with image labels (fully supervised on a huge dataset)\n",
        "\n",
        "* Finetune on the downstream dataset for image classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            dim,\n",
        "            num_heads,\n",
        "            mlp_ratio=4,  # ratio between hidden_dim and input_dim in MLP\n",
        "            act_layer=nn.GELU,\n",
        "            norm_layer=nn.LayerNorm\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = nn.MultiheadAttention(dim, num_heads)\n",
        "        self.norm2 = norm_layer(dim)\n",
        "\n",
        "        hidden_dim = dim * mlp_ratio\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            act_layer(),\n",
        "            nn.Linear(hidden_dim, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Multi-head self-attention mechanism\n",
        "        attn_output, _ = self.attn(self.norm1(x), self.norm1(x), self.norm1(x))\n",
        "        # Add the output of attention mechanism to the input (with normalization)\n",
        "        x = x + attn_output\n",
        "\n",
        "        # MLP\n",
        "        mlp_output = self.mlp(self.norm2(x))\n",
        "        # Add the output of MLP to the input (with normalization)\n",
        "        x = x + mlp_output\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "_zRCYO0UDsa2"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "qz0psf5xXizv"
      },
      "outputs": [],
      "source": [
        "depth = 12\n",
        "many_layers = nn.Sequential(*[Block(128, 8) for _ in range(depth)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ojnWg-sFAMp"
      },
      "source": [
        "![](vit.webp)\n",
        "\n",
        "- CLS token: an extra learnable token\n",
        "- Position embeddings: `x = x + pos_embedding`, where `pos_embedding` is trained for every element is a sequence"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rw4vAMt2kosm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(\n",
        "                    self,\n",
        "                    img_size=(224, 224),\n",
        "                    patch_size=16,\n",
        "                    in_chans=3,\n",
        "                    num_classes=10,\n",
        "                    embed_dim=768,\n",
        "                    depth=12,\n",
        "                    num_heads=12,\n",
        "                    mlp_ratio=4,\n",
        "                    norm_layer=nn.LayerNorm,\n",
        "                    act_layer=nn.GELU\n",
        "            ):\n",
        "        # Your code is here\n",
        "        # Initialize instance variables.\n",
        "\n",
        "        # Size of patches used for tokenization.\n",
        "\n",
        "        # Sequential container for the Transformer blocks.\n",
        "\n",
        "        # Projection layer for patches.\n",
        "\n",
        "        # Length of positional embeddings.\n",
        "\n",
        "        # Learnable token for classification.\n",
        "\n",
        "        # Linear layer for classification.\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            Block(embed_dim, num_heads, mlp_ratio, act_layer, norm_layer) for _ in range(depth)\n",
        "        ])\n",
        "        self.patch_proj = nn.Linear(3 * patch_size * patch_size, embed_dim)\n",
        "        self.embed_len = (img_size[0] * img_size[1]) // (patch_size * patch_size)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, self.embed_len, embed_dim) * .02)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Args:\n",
        "            x: (batch_size, in_channels, img_size[0], img_size[1])\n",
        "\n",
        "        Return:\n",
        "            (batch_size, num_classes)\n",
        "\n",
        "        '''\n",
        "        # Convert input image into patches.\n",
        "\n",
        "        # Project patches into the embedding space.\n",
        "\n",
        "        # Add positional embeddings.\n",
        "\n",
        "        # Add classification token.\n",
        "\n",
        "        # Pass through Transformer blocks.\n",
        "\n",
        "        # Extract only the CLS token.\n",
        "\n",
        "        # Pass the CLS token through the classification layer.\n",
        "        x = img2patches(x, patch_size=self.patch_size)\n",
        "        x = self.patch_proj(x)\n",
        "        x = x + self.pos_embed\n",
        "        x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
        "        x = self.blocks(x)\n",
        "        x = x[:, 0, :]  # take CLS token\n",
        "        return self.head(x)"
      ],
      "metadata": {
        "id": "if82y-h6F-_E"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "nySebNJ1A_8U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0030a709-ef52-4b5c-a470-109caf2506a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "ViT()(torch.ones(5, 3, 224, 224)).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/lucidrains/vit-pytorch"
      ],
      "metadata": {
        "id": "PwgmrRGCxHlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vit-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lW9f_xf_xFne",
        "outputId": "837f2b7d-ed46-47bd-af3a-b732c562a7d3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vit-pytorch\n",
            "  Downloading vit_pytorch-1.6.5-py3-none-any.whl (100 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/100.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m92.2/100.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: einops>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from vit-pytorch) (0.7.0)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from vit-pytorch) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from vit-pytorch) (0.17.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit-pytorch) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit-pytorch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit-pytorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit-pytorch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit-pytorch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10->vit-pytorch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10->vit-pytorch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10->vit-pytorch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10->vit-pytorch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10->vit-pytorch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10->vit-pytorch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10->vit-pytorch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10->vit-pytorch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10->vit-pytorch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10->vit-pytorch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10->vit-pytorch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit-pytorch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10->vit-pytorch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->vit-pytorch) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->vit-pytorch) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->vit-pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->vit-pytorch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, vit-pytorch\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 vit-pytorch-1.6.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from vit_pytorch import ViT\n",
        "\n",
        "v = ViT(\n",
        "    image_size = 256,\n",
        "    patch_size = 32,\n",
        "    num_classes = 1000,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 256, 256)\n",
        "\n",
        "preds = v(img) # (1, 1000)"
      ],
      "metadata": {
        "id": "nHf2w7JAxLT9"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiC6Py2jJfFG",
        "outputId": "253669c6-5f0f-473d-cecb-432b36778b67"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.0762e-02, -8.6543e-01, -1.6649e-01,  4.7023e-01,  2.1664e-01,\n",
              "          2.3016e-01,  8.7242e-02, -6.8354e-02,  4.6087e-01,  6.1433e-01,\n",
              "          9.7971e-01, -5.7961e-01,  2.3350e-01, -6.0116e-01,  6.5677e-01,\n",
              "         -8.1762e-01, -5.7986e-01, -4.3245e-01,  8.9714e-01, -1.9794e-01,\n",
              "         -2.4843e-01, -8.0247e-01,  9.4122e-01,  7.1552e-01, -2.6166e-01,\n",
              "          2.9301e-01, -5.6692e-01, -6.0118e-01, -1.9850e-01,  9.7364e-01,\n",
              "          1.5168e-01, -9.4772e-01, -4.7470e-02, -1.0317e+00,  1.3323e-01,\n",
              "         -5.9982e-01, -6.5861e-01, -4.6528e-01, -4.3678e-01,  5.9566e-01,\n",
              "         -2.3113e-01, -3.2104e-01, -5.0885e-01,  3.9568e-01, -1.2112e-01,\n",
              "          3.5049e-01,  6.6690e-01, -3.8649e-01,  2.5779e-02,  3.7177e-02,\n",
              "         -7.9264e-01,  1.7746e-01,  6.6669e-01,  1.1157e-01,  9.8442e-02,\n",
              "         -8.0961e-01, -1.0259e+00,  7.1363e-02,  7.9590e-01,  2.7726e-01,\n",
              "          3.1313e-01, -3.7796e-01, -6.6821e-01, -1.4317e+00, -2.4080e-01,\n",
              "         -2.3442e-02,  3.9605e-01,  5.7764e-01, -7.3163e-01,  1.5330e-01,\n",
              "          5.2272e-01, -3.8274e-01,  1.6428e-01, -2.3810e-01, -7.0797e-01,\n",
              "         -1.2678e+00,  1.3437e+00,  4.6781e-01, -4.4940e-02,  1.1151e-01,\n",
              "         -1.7016e-01,  9.6493e-01,  3.9252e-01, -1.5116e-01, -1.1441e+00,\n",
              "          8.8831e-01,  1.0620e-02,  1.6964e-01, -3.4007e-01, -4.1188e-01,\n",
              "          1.7661e-01,  1.7657e-01,  3.0911e-01, -1.1834e+00,  4.6574e-01,\n",
              "          5.9294e-03,  9.5356e-02,  4.7775e-01,  1.1345e-01, -4.0620e-01,\n",
              "         -3.7457e-01, -4.4607e-01, -1.0867e+00,  3.1607e-01, -3.6460e-01,\n",
              "         -2.0953e-01, -4.2583e-01, -2.3794e-01,  9.2469e-01, -2.6052e-01,\n",
              "         -1.0706e-01,  9.0274e-01,  6.4078e-01,  7.2602e-01, -1.4250e-01,\n",
              "          5.8122e-01, -2.2541e-01, -3.0910e-01,  1.6689e-01,  5.5735e-01,\n",
              "          3.0262e-01, -1.2529e+00, -2.1135e-01,  6.6500e-01, -8.5536e-02,\n",
              "         -2.3301e-01,  4.1921e-02,  1.1451e-01,  4.3707e-02,  5.8103e-01,\n",
              "          6.0641e-01, -9.6656e-02,  5.4536e-02, -3.6121e-01, -8.9279e-01,\n",
              "         -4.2918e-01, -3.2265e-01,  1.8493e-02, -5.3678e-01,  4.8098e-01,\n",
              "         -3.1798e-02,  5.9988e-01,  3.2797e-03, -1.0095e-01, -2.0685e-01,\n",
              "         -4.5599e-01, -9.0330e-01, -7.4570e-01,  4.3097e-01,  2.6359e-01,\n",
              "         -3.0216e-01,  3.7139e-02,  6.0048e-01,  6.2854e-01,  7.3685e-01,\n",
              "         -1.4188e-01, -4.7483e-01, -8.8145e-02, -2.4934e-01,  7.6390e-02,\n",
              "         -1.3249e+00,  4.1879e-01, -2.3618e-01, -7.4500e-01,  2.0335e-01,\n",
              "         -1.4629e-01,  1.1602e+00,  2.5923e-02, -3.8514e-01, -9.2864e-01,\n",
              "          5.9825e-01, -4.6737e-01, -3.5544e-02,  8.1000e-02,  1.0953e+00,\n",
              "         -6.6863e-01, -1.4375e+00,  4.8564e-01,  1.0976e+00, -6.0233e-01,\n",
              "          9.1241e-01,  2.1592e-01, -1.8760e-01,  7.6062e-02,  1.1036e+00,\n",
              "          1.0444e-01,  9.3224e-02,  2.1648e-01,  1.1198e-01, -1.2268e+00,\n",
              "         -5.9617e-02, -1.9867e-01,  1.0603e-01,  9.4137e-03, -2.9897e-01,\n",
              "         -7.7564e-01,  8.2043e-01, -1.9994e-01,  7.6935e-01, -1.0958e+00,\n",
              "          6.8690e-01,  2.5506e-01,  6.2486e-01,  1.4662e-01, -6.3332e-01,\n",
              "         -2.1446e-01, -4.6303e-01, -7.6422e-01, -1.2919e-01,  5.1818e-03,\n",
              "         -1.0193e+00, -7.5174e-01,  6.5751e-01, -5.5069e-01,  1.4381e-01,\n",
              "          3.3718e-01, -6.2939e-01, -1.7363e-01, -2.2079e-01,  6.4239e-01,\n",
              "         -2.3523e-01,  6.1799e-01,  9.4609e-01, -6.4031e-01, -6.4893e-01,\n",
              "          5.1212e-02,  9.5674e-02, -1.2049e+00,  9.7552e-02,  9.7635e-02,\n",
              "          3.3162e-01, -1.0921e-01,  4.5625e-01, -1.2396e+00, -1.6015e-01,\n",
              "          1.1293e+00,  5.9282e-01,  2.9897e-01,  2.4668e-01, -2.8455e-01,\n",
              "          1.4684e+00,  5.9888e-01,  5.1893e-01, -2.3494e-01,  2.8938e-03,\n",
              "         -1.5398e-01,  3.7207e-01,  9.2539e-02,  9.8638e-01,  3.8862e-01,\n",
              "          3.5257e-01,  8.1810e-01,  9.2813e-01, -3.8975e-01,  6.5053e-01,\n",
              "          8.1386e-01,  6.6365e-01, -6.1812e-02, -4.6974e-01,  9.0077e-02,\n",
              "          4.8629e-01,  6.8969e-01,  8.7541e-01,  7.0156e-01,  6.0753e-01,\n",
              "          1.3260e+00,  5.3494e-01,  5.8991e-01, -4.6032e-01,  9.1758e-01,\n",
              "          5.0949e-01, -8.5209e-01,  4.7969e-01, -8.8391e-01,  3.4718e-01,\n",
              "          6.0903e-01,  3.5848e-01,  3.6204e-01,  2.6591e-01,  1.9701e-01,\n",
              "          4.7105e-01,  1.2563e+00,  2.1160e-01, -4.0015e-01,  6.9075e-01,\n",
              "          7.6901e-01, -5.9638e-01,  1.8211e+00, -1.0293e-01, -5.2010e-01,\n",
              "          2.2028e-03, -5.0733e-01,  7.7440e-01,  5.6728e-01,  2.6887e-01,\n",
              "          5.6666e-01,  4.5875e-02,  7.9626e-01, -1.0549e+00, -6.5999e-01,\n",
              "         -1.1074e+00, -4.2208e-02, -2.0691e-01,  2.5095e-01,  3.2980e-02,\n",
              "         -4.5361e-01,  2.3734e-01, -6.8357e-02, -2.1987e-02, -4.1915e-01,\n",
              "         -4.5095e-01, -4.1147e-01,  2.3276e-01, -1.2058e-02,  1.7534e-01,\n",
              "         -2.5620e-02,  1.9292e-02,  8.1358e-01, -5.8155e-01,  2.4652e-01,\n",
              "         -4.0021e-02,  6.7850e-01,  7.1035e-01,  4.8545e-01,  2.7878e-01,\n",
              "         -2.5453e-02, -3.3243e-01,  7.8494e-01,  1.0005e+00, -6.2398e-01,\n",
              "         -3.6908e-01, -7.4968e-01, -1.4239e-01,  6.0538e-01,  2.0840e-01,\n",
              "         -6.5618e-01,  1.3055e+00, -1.3955e-01, -3.8349e-02,  2.4660e-01,\n",
              "          4.7573e-01, -6.4538e-01, -9.4334e-01,  7.3394e-01, -1.3454e-01,\n",
              "          6.1120e-01, -1.0066e-01, -2.3646e-01,  3.6862e-01,  2.3302e-01,\n",
              "          1.0715e+00, -2.7426e-01, -3.3668e-01,  7.1916e-01, -7.2137e-01,\n",
              "         -2.7365e-01, -4.2246e-01,  6.9133e-01,  3.6446e-01, -1.0145e+00,\n",
              "         -6.6128e-01, -1.0647e+00, -4.3472e-01,  6.0212e-01, -3.9317e-01,\n",
              "          7.1249e-02,  2.2932e-01, -6.8380e-01,  5.0188e-01,  2.3552e-01,\n",
              "         -5.0960e-01,  7.4117e-01, -7.7968e-01,  5.2973e-01, -1.2118e+00,\n",
              "          1.5200e+00,  1.7133e-01,  7.7341e-01, -2.6486e-01, -2.2649e-01,\n",
              "         -3.2417e-01, -3.0693e-01, -2.6091e-01, -9.9909e-01,  5.6551e-01,\n",
              "         -6.2739e-03,  1.2885e-01,  5.0128e-01,  1.4646e-01, -5.8029e-01,\n",
              "          4.2155e-01,  2.2588e-02,  1.1914e+00, -1.5855e-01,  6.2997e-01,\n",
              "         -7.9413e-01,  4.9885e-01,  2.5285e-02,  1.7892e-01,  5.5246e-02,\n",
              "          1.4002e+00,  9.2504e-02, -1.7995e-02, -4.9662e-03,  2.4345e-01,\n",
              "          2.5456e-01, -1.3928e+00,  7.4174e-02, -7.2395e-01, -2.9096e-01,\n",
              "          2.8982e-01,  1.0887e-01,  5.5796e-01, -4.1070e-01,  1.4233e-01,\n",
              "         -1.4405e-01,  1.0964e+00, -2.7354e-01, -4.3965e-01,  9.1207e-01,\n",
              "          4.1277e-01, -3.5195e-01,  7.7876e-01, -4.0184e-01,  1.2797e-01,\n",
              "          4.1948e-01,  2.1123e-01,  5.9506e-01, -4.0275e-01, -2.3102e-01,\n",
              "          2.0323e-01, -2.5792e-01, -3.6271e-01, -1.5123e-03, -1.0116e+00,\n",
              "          1.9626e-01,  2.9145e-01,  2.1923e-01,  3.9955e-01,  7.1228e-02,\n",
              "          3.7031e-01, -3.4862e-01,  3.6890e-01, -6.4080e-01,  8.1026e-02,\n",
              "         -1.8717e-01, -6.3531e-02, -6.4584e-01,  3.5130e-01,  9.2283e-01,\n",
              "         -4.5420e-01, -3.4739e-01, -7.9950e-01,  9.5004e-01, -4.9713e-01,\n",
              "         -8.0162e-01,  6.3422e-01,  4.3825e-01, -2.6727e-01,  6.1130e-01,\n",
              "         -6.6202e-01, -2.2075e-01, -1.4528e-01, -6.6555e-01, -1.3067e-02,\n",
              "         -2.9035e-01, -1.2444e-01,  7.5649e-02, -8.6257e-01,  7.1656e-02,\n",
              "         -3.4373e-01,  3.2625e-01, -2.4365e-01,  5.2682e-01,  4.8571e-02,\n",
              "          4.4809e-01, -3.5095e-02, -3.3775e-01, -9.8795e-01,  4.9058e-01,\n",
              "          9.9829e-01, -7.0488e-01,  4.4753e-01,  6.2767e-01,  9.2961e-01,\n",
              "         -6.8682e-01, -5.2319e-01, -4.1370e-01,  1.5137e-01,  8.8074e-01,\n",
              "          3.8477e-01, -5.5568e-01, -1.2034e-01,  2.4953e-01, -5.7424e-01,\n",
              "         -1.9051e-01,  7.2689e-01, -6.7920e-01, -3.2147e-01, -1.9574e-01,\n",
              "          2.4681e-01,  6.2672e-01,  1.3293e-01, -9.7622e-01,  4.4519e-01,\n",
              "         -6.6002e-01,  4.3809e-01, -1.0845e-01,  2.6866e-01,  1.0162e+00,\n",
              "         -2.2675e-01,  4.6134e-01,  3.9990e-01, -4.3922e-01, -6.9113e-02,\n",
              "          8.7370e-01, -1.3939e-01, -1.4777e-02,  2.9269e-01,  2.1055e-01,\n",
              "          5.9016e-02, -7.9948e-01, -5.0617e-01, -1.3524e+00,  2.9790e-01,\n",
              "          5.7962e-01,  1.0301e+00,  1.2052e+00,  3.4638e-01,  3.3890e-01,\n",
              "          3.5654e-01,  5.5081e-01, -3.0326e-01,  2.2315e-01, -6.0486e-01,\n",
              "          3.3650e-01, -8.3146e-01, -3.0409e-02,  7.0797e-01, -6.0304e-01,\n",
              "          1.4168e-01,  2.6357e-02, -2.1899e-02, -3.7630e-01,  5.2053e-01,\n",
              "          2.8809e-01, -7.3008e-01, -6.6021e-01, -3.1615e-01,  2.8247e-02,\n",
              "         -5.4110e-01, -1.1649e-01,  2.5144e-01,  8.5214e-01,  1.0400e+00,\n",
              "          3.8968e-01, -8.6697e-01,  2.2795e-01,  1.5531e-01, -6.8525e-02,\n",
              "          2.5613e-01, -4.4999e-01, -8.5440e-01, -2.2863e-02, -2.2916e-01,\n",
              "         -2.2397e-01, -2.4874e-01,  3.2661e-01, -1.5702e-01,  3.4661e-01,\n",
              "          9.9465e-01, -2.0027e-01, -1.9652e-01, -2.6566e-01, -1.0992e-01,\n",
              "          8.6820e-01,  1.9350e-01, -4.6484e-01,  7.1946e-02, -2.7211e-01,\n",
              "          8.0221e-01,  7.4395e-02, -2.2606e-01,  4.4106e-01, -1.5178e-01,\n",
              "         -6.2175e-02,  4.7363e-02,  2.9740e-01,  4.0722e-01, -2.1246e-01,\n",
              "          4.6190e-02,  1.6027e-01, -2.3858e-01,  3.2077e-01,  1.0695e+00,\n",
              "          1.3413e-01, -1.0944e+00,  2.8347e-01, -5.1918e-01,  1.2066e-01,\n",
              "          5.4129e-01,  1.3041e-01, -2.5759e-01, -2.9327e-01,  7.0722e-01,\n",
              "          1.8816e-01,  9.5582e-01,  3.8403e-02,  9.4970e-01,  4.5102e-01,\n",
              "          5.4415e-01, -5.7519e-01,  1.9782e-02,  9.4192e-02, -3.9708e-01,\n",
              "          9.0392e-01,  5.8528e-01, -4.6071e-01,  5.3684e-01, -5.2416e-01,\n",
              "         -6.1333e-01, -2.7491e-01,  7.8085e-01,  9.6820e-01, -7.0690e-01,\n",
              "         -6.2893e-01,  2.8734e-01, -4.2681e-01, -4.2096e-01,  1.0797e-01,\n",
              "         -1.2248e-01,  1.3440e+00,  2.8678e-01,  4.9306e-01,  2.7661e-01,\n",
              "          2.8988e-01, -1.3096e-01,  1.6847e-01, -7.8696e-01,  7.6742e-01,\n",
              "         -1.2162e+00,  6.8955e-01,  1.1278e+00, -5.2357e-01, -4.8009e-02,\n",
              "          3.0138e-01,  1.0505e+00,  3.1012e-01,  7.0974e-01,  8.0510e-01,\n",
              "          2.2409e-01,  3.4887e-01, -2.5172e-02, -1.0080e+00, -3.0042e-01,\n",
              "         -1.8208e-02, -1.7355e-01,  3.7298e-01, -5.6714e-01,  3.7428e-01,\n",
              "         -1.3245e-01,  1.3463e-03, -5.6830e-01,  3.4762e-01, -2.3293e-01,\n",
              "          1.0879e-01,  4.6146e-01,  2.1010e-01,  3.3556e-01, -5.7767e-01,\n",
              "          2.6648e-01, -8.4677e-01, -4.0090e-01,  2.0425e-01,  4.6021e-02,\n",
              "          2.9219e-01,  1.5405e-01,  1.1547e+00, -4.3729e-01, -5.5990e-01,\n",
              "         -2.4184e-01,  8.1354e-01,  3.8569e-01,  8.8435e-02, -5.4979e-02,\n",
              "         -1.3739e+00,  1.0311e+00, -1.6172e-01, -3.5930e-01,  2.9927e-01,\n",
              "          7.8065e-02,  3.5105e-01, -7.8155e-01,  4.2200e-01,  1.0202e+00,\n",
              "         -5.9546e-02, -8.7257e-01,  2.1146e-01,  9.7909e-01, -7.6884e-01,\n",
              "          5.1504e-02,  3.3637e-01,  2.0398e-01, -1.2718e+00, -2.7672e-01,\n",
              "         -1.1977e-01,  5.5367e-01,  7.4392e-01, -4.4809e-01, -1.0472e+00,\n",
              "          8.1562e-01, -1.8000e-01, -5.3304e-01,  2.4524e-01, -3.0416e-02,\n",
              "          6.1639e-02, -1.6952e-01, -5.0193e-01, -8.3799e-01,  5.2606e-01,\n",
              "          7.7977e-01,  7.5173e-01, -2.2557e-01,  3.5468e-01,  7.0928e-01,\n",
              "         -1.2505e-01,  3.9086e-01, -1.0075e+00,  6.1595e-01,  4.8882e-01,\n",
              "         -3.3365e-01, -6.6357e-01,  1.9445e-01, -8.4119e-02,  1.0160e+00,\n",
              "          8.5142e-02,  1.3573e+00, -1.2192e-01,  8.5866e-02, -5.7818e-01,\n",
              "          2.9188e-02, -6.5496e-02, -1.3885e-01, -2.0008e-02,  7.6588e-01,\n",
              "         -8.7641e-01, -5.3399e-01,  6.9993e-01, -3.7952e-01,  2.6665e-01,\n",
              "         -2.6444e-02,  2.3982e-01, -6.2466e-01,  2.5912e-01, -1.1418e-01,\n",
              "          7.4858e-01,  2.3111e-01,  5.5765e-02, -1.4011e-01, -1.2102e+00,\n",
              "          6.1690e-01, -5.3366e-01, -5.1483e-02,  2.2177e-01,  3.4894e-01,\n",
              "          8.0160e-01,  7.3246e-01,  2.5179e-01, -1.6994e-01,  4.1103e-01,\n",
              "         -2.8596e-01,  2.0393e-02, -2.8970e-01,  1.6733e-01, -3.4087e-01,\n",
              "         -3.6277e-01,  4.1102e-01,  2.3255e-02,  2.4215e-01, -3.3573e-02,\n",
              "         -2.3464e-01, -4.4021e-01, -3.9550e-01, -4.3466e-01, -5.4914e-01,\n",
              "          7.7268e-03,  7.7036e-01,  9.4748e-01, -3.6552e-01,  1.3179e-01,\n",
              "         -1.7048e+00, -4.0625e-01, -1.9150e-02,  5.0019e-01,  1.0255e+00,\n",
              "          7.5830e-01,  1.2988e+00, -7.0751e-01, -9.7823e-02, -1.7807e-01,\n",
              "         -8.2350e-01,  5.3553e-02, -1.8177e+00, -1.0058e+00, -3.3938e-01,\n",
              "          7.4009e-02,  4.8068e-01, -1.4038e-02, -8.3893e-01, -6.1712e-02,\n",
              "          4.2391e-02,  8.8265e-01,  5.2506e-01,  2.5493e-01,  3.6749e-01,\n",
              "         -1.2255e-01, -5.2917e-02,  1.0623e+00,  8.9673e-01,  9.2835e-02,\n",
              "         -5.2302e-01,  2.7788e-01,  5.6156e-01, -2.3390e-01, -5.2653e-01,\n",
              "         -4.1711e-01, -2.7355e-01, -5.1592e-01,  1.0180e+00, -2.4186e-01,\n",
              "         -9.9618e-01,  4.7919e-01, -3.6204e-01,  7.0408e-02,  2.3424e-01,\n",
              "          1.9269e-01, -4.4562e-01, -2.8388e-01, -4.7973e-02,  2.5798e-01,\n",
              "          3.7393e-01, -1.7272e+00, -1.4155e-01,  4.8240e-01,  8.1078e-01,\n",
              "         -5.0322e-01,  2.4802e-01,  2.1395e-01,  1.3771e-01,  4.3107e-01,\n",
              "          2.7728e-01,  5.8320e-01,  4.6503e-02, -1.0599e+00, -7.6032e-01,\n",
              "         -3.9790e-01, -1.2738e+00,  6.0391e-01,  3.3331e-01, -1.4248e+00,\n",
              "          1.2005e+00,  2.9912e-01,  1.7835e-01,  4.3687e-01,  1.2031e-01,\n",
              "          3.2008e-01,  5.6315e-01, -4.9975e-01, -8.1657e-01, -1.8459e-01,\n",
              "          4.3092e-01,  2.0623e-01,  5.2601e-01,  1.0486e+00,  1.4799e-01,\n",
              "          6.3885e-01, -1.2318e-01,  2.6224e-01,  7.2861e-02,  1.3762e+00,\n",
              "         -3.3409e-01, -6.8131e-01, -5.8798e-01, -2.8419e-01, -1.0337e-01,\n",
              "          4.1466e-01, -2.1436e-01, -6.2626e-01, -5.1471e-01, -5.2495e-01,\n",
              "         -3.7714e-01,  1.4507e+00, -5.7106e-01,  1.5770e+00, -4.4811e-01,\n",
              "         -2.9375e-01,  3.3637e-02,  7.4487e-01, -8.5880e-01,  3.1812e-02,\n",
              "         -8.7008e-01, -6.8360e-01, -2.4978e-01, -5.2781e-01,  8.8304e-01,\n",
              "         -1.4572e-01, -6.9429e-01, -2.8658e-01,  3.9883e-02, -1.2283e-01,\n",
              "         -5.4724e-02, -1.4398e-01, -1.1650e-01, -3.0273e-02, -2.6887e-01,\n",
              "          1.0129e+00, -1.8756e-01,  7.8935e-01, -4.8023e-01,  1.7266e+00,\n",
              "          4.4839e-02,  2.0548e-02, -3.2378e-01, -2.5402e-01, -4.8742e-01,\n",
              "          1.8802e-01,  1.1646e+00, -5.5122e-01, -1.1163e-01,  6.2729e-01,\n",
              "          3.7860e-01, -1.2485e-01, -4.9820e-01, -3.6015e-01,  8.6031e-01,\n",
              "         -6.1817e-01,  3.7659e-01, -6.0268e-01,  6.1336e-01,  3.5857e-01,\n",
              "          1.1177e-01,  3.1385e-01, -9.3736e-01,  8.0572e-01,  5.7675e-01,\n",
              "          8.4122e-01, -4.9096e-01,  6.0328e-02, -1.1383e+00, -2.8073e-01,\n",
              "         -1.2021e+00,  1.7345e-01, -8.0979e-01, -2.2243e-01,  6.8274e-01,\n",
              "          4.0912e-01, -1.2245e+00, -1.2196e-01, -5.3782e-01, -1.2546e-02,\n",
              "         -9.6026e-01, -6.0359e-01,  7.0244e-01,  1.2354e-01,  5.5343e-01,\n",
              "         -3.7668e-01,  1.2770e-01, -2.5510e-01,  4.5273e-01, -1.3526e-01,\n",
              "         -4.9048e-01,  4.1719e-01,  3.2739e-01,  3.1032e-01,  2.7297e-01,\n",
              "          7.7414e-02,  2.8138e-01,  2.6746e-01,  2.9843e-01,  4.7441e-01,\n",
              "          7.8972e-01,  5.8042e-01, -4.0025e-01,  6.0415e-02,  3.6478e-01,\n",
              "         -8.8883e-01,  2.8299e-01,  5.6490e-01,  1.8372e-01,  8.1750e-01,\n",
              "         -6.1662e-01,  6.1520e-01, -3.3852e-01, -2.9883e-01, -2.2590e-01,\n",
              "          3.2326e-01, -2.5042e-02,  4.8560e-01, -2.6339e-01, -7.7366e-01]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IZU9hnF8JfKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvP_QijO89xm"
      },
      "source": [
        "# HuggingFace\n",
        "\n",
        "Hugging Face is an open-source library that provides easy access to state-of-the-art transformer-based models for NLP tasks. It offers a comprehensive set of tools for working with these models, including loading pre-trained models, fine-tuning on custom datasets, and deploying models for inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcsmaM7wiY5z"
      },
      "source": [
        "### Getting started on a task with a pipeline\n",
        "\n",
        "The easiest way to use a pre-trained model on a given task is to use pipeline(). 🤗 Transformers provides the following tasks out of the box:\n",
        "Sentiment analysis: is a text positive or negative?\n",
        "\n",
        "1. Text generation: provide a prompt and the model will generate what follows.\n",
        "2. Name entity recognition (NER): in an input sentence, label each word with the entity it represents (person, place, etc.)\n",
        "3. Question answering: provide the model with some context and a question, extract the answer from the context.\n",
        "4. Filling masked text: given a text with masked words (e.g., replaced by [MASK]), fill the blanks.\n",
        "5. Summarization: generate a summary of a long text.\n",
        "6. Language Translation: translate a text into another language.\n",
        "7. Feature extraction: return a tensor representation of the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVJUSak5F1tz"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nkiMQgwirgo"
      },
      "source": [
        "### GPT2\n",
        "\n",
        "#### Model description\n",
        "\n",
        "**GPT-2** is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences.\n",
        "\n",
        "More precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence, shifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the predictions for the token i only uses the inputs from 1 to i but not the future tokens.\n",
        "\n",
        "This way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhEJJK36jApe"
      },
      "source": [
        "### Text generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5CeZ7c_jE76"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oh029QYxjK6l"
      },
      "outputs": [],
      "source": [
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)\n",
        "generator(\"Hello, in this seminar we will learn how to,\", max_length=60, num_return_sequences=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPKRx9Z1jQnx"
      },
      "outputs": [],
      "source": [
        "generator(\"Machine learning is evolving technology\", max_length=10, num_return_sequences=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTkd8VcUjYMv"
      },
      "source": [
        "### Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YabWA_FSjZZs"
      },
      "outputs": [],
      "source": [
        "# Allocate a pipeline for sentiment-analysis\n",
        "classifier = pipeline('sentiment-analysis')\n",
        "classifier('The weather is awesome!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0tXsP-LkVym"
      },
      "source": [
        "### Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrIE3oGfkXF2"
      },
      "outputs": [],
      "source": [
        "# Allocate a pipeline for question-answering\n",
        "question_answerer = pipeline('question-answering')\n",
        "question_answerer({\n",
        "    'question': 'What is the Newtons third law of motion?',\n",
        "    'context': 'Newton’s third law of motion states that, \"For every action there is equal and opposite reaction\"'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xO8tP9oEkg7C"
      },
      "outputs": [],
      "source": [
        "nlp = pipeline(\"question-answering\")\n",
        "\n",
        "context = r\"\"\"\n",
        "Micorsoft was founded by Bill gates and Paul allen in the year 1975.\n",
        "The property of being prime (or not) is called primality.\n",
        "A simple but slow method of verifying the primality of a given number n is known as trial division.\n",
        "It consists of testing whether n is a multiple of any integer between 2 and itself.\n",
        "Algorithms much more efficient than trial division have been devised to test the primality of large numbers.\n",
        "These include the Miller–Rabin primality test, which is fast but has a small probability of error, and the AKS primality test, which always produces the correct answer in polynomial time but is too slow to be practical.\n",
        "Particularly fast methods are available for numbers of special forms, such as Mersenne numbers.\n",
        "As of January 2016, the largest known prime number has 22,338,618 decimal digits.\n",
        "\"\"\"\n",
        "\n",
        "#Question 1\n",
        "result = nlp(question=\"What is a simple method to verify primality?\", context=context)\n",
        "\n",
        "print(f\"Answer 1: '{result['answer']}'\")\n",
        "\n",
        "#Question 2\n",
        "result = nlp(question=\"When did Bill gates founded Microsoft?\", context=context)\n",
        "\n",
        "print(f\"Answer 2: '{result['answer']}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Jm08cOsV_Yx"
      },
      "source": [
        "### BERT\n",
        "\n",
        "The BERT model was proposed in BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It’s a bidirectional transformer pretrained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.\n",
        "\n",
        "The abstract from the paper is the following:\n",
        "\n",
        "> We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n",
        "\n",
        "BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQpuivjmlXMa"
      },
      "source": [
        "### Text prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjNxlDyPlTUV"
      },
      "outputs": [],
      "source": [
        "unmasker = pipeline('fill-mask', model='bert-base-cased')\n",
        "unmasker(\"Hello, My name is [MASK].\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKEpbCM1lkhc"
      },
      "source": [
        "### Text Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewk4_wLEl064"
      },
      "outputs": [],
      "source": [
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "ARTICLE = \"\"\"The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which accomplished landing the first humans on the Moon from 1969 to 1972.\n",
        "First conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space,\n",
        "Apollo was later dedicated to President John F. Kennedy's national goal of \"landing a man on the Moon and returning him safely to the Earth\" by the end of the 1960s, which he proposed in a May 25, 1961, address to Congress.\n",
        "Project Mercury was followed by the two-man ProjectGemini (1962–66).\n",
        "The first manned flight of Apollo was in 1968.\n",
        "Apollo ran from 1961 to 1972, and was supported by the two-man Gemini program which ran concurrently with it from 1962 to 1966.\n",
        "Gemini missions developed some of the space travel techniques that were necessary for the success of the Apollo missions.\n",
        "Apollo used Saturn family rockets as launch vehicles.\n",
        "Apollo/Saturn vehicles were also used for an Apollo Applications Program, which consisted of Skylab, a space station that supported three manned missions in 1973–74, and the Apollo–Soyuz Test Project, a joint Earth orbit mission with the Soviet Union in 1975.\n",
        " \"\"\"\n",
        "\n",
        "summary=summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False)[0]\n",
        "\n",
        "print(summary['summary_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Sj_V48FpWTy"
      },
      "source": [
        "### English to German translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JP2pfiUvpaGp"
      },
      "outputs": [],
      "source": [
        "# English to German\n",
        "translator_ger = pipeline(\"translation_en_to_de\")\n",
        "print(\"German: \",translator_ger(\"Joe Biden became the 46th president of U.S.A.\", max_length=40)[0]['translation_text'])\n",
        "\n",
        "# English to French\n",
        "translator_fr = pipeline('translation_en_to_fr')\n",
        "print(\"French: \",translator_fr(\"Joe Biden became the 46th president of U.S.A\",  max_length=40)[0]['translation_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4mpWPgPqPwY"
      },
      "source": [
        "### Fill MASK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKbMqtrxciSb"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
        "unmasker(\"Artificial Intelligence [MASK] take over the world.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujIGeQrvGIhI"
      },
      "source": [
        "In conclusion, Hugging Face provides a user-friendly interface for working with transformer-based models in NLP. We've covered how to load pre-trained models, fine-tune them for specific tasks, and even implement custom transformers. With its extensive documentation and active community, Hugging Face is an invaluable tool for NLP practitioners."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTVjMJiOBMv2"
      },
      "source": [
        "# Mamba\n",
        "\n",
        "<img src=\"https://github.com/state-spaces/mamba/blob/main/assets/selection.png?raw=true\" alt=\"Mamba\" >"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbtckhZLBCpc"
      },
      "outputs": [],
      "source": [
        "!pip install causal-conv1d>=1.2.0\n",
        "!pip install mamba-ssm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJkr9JaCCYXj"
      },
      "outputs": [],
      "source": [
        "from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-2.8b-hf\")\n",
        "model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-2.8b-hf\")\n",
        "input_ids = tokenizer(\"LOOK!\", return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "out = model.generate(input_ids, max_new_tokens=10)\n",
        "print(tokenizer.batch_decode(out))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXofEUBdDx5g"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PhD",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7a4b3960547ae30807a4950c470fa8e07d0754b1ba6745fcabd143662a5dd1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}